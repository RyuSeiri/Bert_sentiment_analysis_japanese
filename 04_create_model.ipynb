{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc96deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "from transformers import DistilBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2b457c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 110\n",
    "output_file = \"data.tfrecord\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c3832521",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\orignal_data.pkl\", \"rb\") as myprofile:  \n",
    "    dt = pickle.load(myprofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "61224d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n",
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Some layers from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing TFDistilBertForSequenceClassification: ['bert', 'nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['pre_classifier', 'classifier', 'distilbert', 'dropout_125']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking', num_labels=2)\n",
    "#.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78673951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self,model):\n",
    "        super().__init__()\n",
    "        #self.model = model.distilbert(input_ids,input_mask)[0]\n",
    "        self.model = model\n",
    "        self.model.trainable = False\n",
    "        self.layersG = tf.keras.layers.GlobalMaxPool1D()\n",
    "        self.layers1 = tf.keras.layers.Dense(50, activation=\"relu\")\n",
    "        self.layersD2 = tf.keras.layers.Dropout(0.2)\n",
    "        self.layers3 = tf.keras.layers.Dense(10, activation=\"relu\")\n",
    "        self.layersD4 = tf.keras.layers.Dropout(0.2)\n",
    "        self.layers5 = tf.keras.layers.Dense(2, activation=\"softmax\")\n",
    "        self.Bidirectional=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(60, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
    "    def call(self, inputs):\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        input_mask = inputs[\"input_mask\"]\n",
    "        embedding_layer  = self.model.distilbert(input_ids,input_mask)[0]\n",
    "        X =self.Bidirectional(embedding_layer)\n",
    "        X = self.layersG(X)\n",
    "        X = self.layers1(X)\n",
    "        X = self.layersD2(X)\n",
    "        X = self.layers3(X)\n",
    "        X = self.layersD4(X)\n",
    "        X = self.layers5(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4796e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output_file = \"train_data.tfrecord\"\n",
    "test_output_file = \"test_data.tfrecord\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2e518f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data_and_label_from_record(record):\n",
    "    x = {\n",
    "        \"input_ids\": record[\"input_ids\"],\n",
    "        \"input_mask\": record[\"input_mask\"],\n",
    "        # 'segment_ids': record['segment_ids']\n",
    "    }\n",
    "    y = record[\"label_ids\"]\n",
    "    return (x, y)\n",
    "def _decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    return tf.io.parse_single_example(record, name_to_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2d20b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_data(file_name,isTrain = False):\n",
    "    dataset = tf.data.TFRecordDataset(\"data.tfrecord\")\n",
    "    if isTrain :\n",
    "        dataset = dataset.repeat(500)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    name_to_features = {\n",
    "            \"input_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "            \"input_mask\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "            # \"segment_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
    "            \"label_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "    drop_remainder=False\n",
    "    dataset = dataset.apply(\n",
    "        tf.data.experimental.map_and_batch(\n",
    "            lambda record: _decode_record(record, name_to_features),\n",
    "            batch_size=100,\n",
    "            drop_remainder=drop_remainder,\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "        )\n",
    "    )\n",
    "    dataset.cache()\n",
    "    re_dataset = dataset.map(select_data_and_label_from_record)\n",
    "    return re_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "56b0ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_train_test_data(train_output_file,True)\n",
    "test_dataset = create_train_test_data(test_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "80000196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=({'input_ids': TensorSpec(shape=(None, 110), dtype=tf.int64, name=None), 'input_mask': TensorSpec(shape=(None, 110), dtype=tf.int64, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fce92022",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-3\n",
    "epsilon = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d6d84b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode2 = MyModel(model)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\n",
    "mode2.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "#mode2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d602df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b372440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20/20 [==============================] - 898s 45s/step - loss: 0.7138 - accuracy: 0.5285 - val_loss: 0.6715 - val_accuracy: 0.6280\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 891s 45s/step - loss: 0.6724 - accuracy: 0.6035 - val_loss: 0.6618 - val_accuracy: 0.6280\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 890s 45s/step - loss: 0.6429 - accuracy: 0.6150 - val_loss: 0.5741 - val_accuracy: 0.6280\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 884s 45s/step - loss: 0.5862 - accuracy: 0.6995 - val_loss: 0.5069 - val_accuracy: 0.8060\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 882s 45s/step - loss: 0.5216 - accuracy: 0.7555 - val_loss: 0.4575 - val_accuracy: 0.8060\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 880s 44s/step - loss: 0.5319 - accuracy: 0.7600 - val_loss: 0.4210 - val_accuracy: 0.8200\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 883s 45s/step - loss: 0.4910 - accuracy: 0.7860 - val_loss: 0.3994 - val_accuracy: 0.8620\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 880s 44s/step - loss: 0.4241 - accuracy: 0.8170 - val_loss: 0.3139 - val_accuracy: 0.8620\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 882s 45s/step - loss: 0.4061 - accuracy: 0.8300 - val_loss: 0.3160 - val_accuracy: 0.8700\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 902s 46s/step - loss: 0.4092 - accuracy: 0.8315 - val_loss: 0.3288 - val_accuracy: 0.8780\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 902s 46s/step - loss: 0.3719 - accuracy: 0.8495 - val_loss: 0.3891 - val_accuracy: 0.7980\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 895s 45s/step - loss: 0.3438 - accuracy: 0.8515 - val_loss: 0.2825 - val_accuracy: 0.8720\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 899s 45s/step - loss: 0.3793 - accuracy: 0.8505 - val_loss: 0.2589 - val_accuracy: 0.9020\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 902s 46s/step - loss: 0.3360 - accuracy: 0.8690 - val_loss: 0.2677 - val_accuracy: 0.8920\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 905s 46s/step - loss: 0.2736 - accuracy: 0.8905 - val_loss: 0.1747 - val_accuracy: 0.9300\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 903s 46s/step - loss: 0.2821 - accuracy: 0.8965 - val_loss: 0.1631 - val_accuracy: 0.9340\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 907s 46s/step - loss: 0.2564 - accuracy: 0.9055 - val_loss: 0.1727 - val_accuracy: 0.9360\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 903s 46s/step - loss: 0.2306 - accuracy: 0.9285 - val_loss: 0.1398 - val_accuracy: 0.9440\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 926s 47s/step - loss: 0.1851 - accuracy: 0.9385 - val_loss: 0.0957 - val_accuracy: 0.9720\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 903s 46s/step - loss: 0.2188 - accuracy: 0.9255 - val_loss: 0.1064 - val_accuracy: 0.9660\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 910s 46s/step - loss: 0.1859 - accuracy: 0.9300 - val_loss: 0.0978 - val_accuracy: 0.9660\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 906s 46s/step - loss: 0.1519 - accuracy: 0.9440 - val_loss: 0.0668 - val_accuracy: 0.9760\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 901s 46s/step - loss: 0.1718 - accuracy: 0.9460 - val_loss: 0.1573 - val_accuracy: 0.9360\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 909s 46s/step - loss: 0.1793 - accuracy: 0.9365 - val_loss: 0.0956 - val_accuracy: 0.9680\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 904s 46s/step - loss: 0.2036 - accuracy: 0.9245 - val_loss: 0.0732 - val_accuracy: 0.9820\n",
      "Epoch 26/50\n",
      "13/20 [==================>...........] - ETA: 4:16 - loss: 0.1903 - accuracy: 0.9292"
     ]
    }
   ],
   "source": [
    "history = mode2.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch = 20,\n",
    "    validation_data=test_dataset,\n",
    "    validation_steps=5,\n",
    "    #shuffle=True,\n",
    "    epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
